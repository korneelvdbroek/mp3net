{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataprep_example.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPVf3oq0n+WoWhLLm0HeEdO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/korneelvdbroek/mp3net/blob/main/colab/dataprep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaTKHxAEa71L"
      },
      "source": [
        "### Description\n",
        "This script takes a set of audio files and prepares them to be used as training data for MP3net. \n",
        "\n",
        "### Usage\n",
        "The script below assumes you store the program code on Google Drive and audio data on gs:// To use this notebook, check the cells below for capitalized tags which you will need to personalize.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbMzRpMoT3hG"
      },
      "source": [
        "# check location of backend\n",
        "import subprocess\n",
        "import json\n",
        "proc=subprocess.Popen('curl ipinfo.io', shell=True, stdout=subprocess.PIPE, )\n",
        "ip_data = json.loads(proc.communicate()[0])\n",
        "server_country = ip_data['country']\n",
        "print(f\"Server location:   {ip_data['city']} ({ip_data['region']}), {server_country}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOgv4chUD96S"
      },
      "source": [
        "project_id = 'YOUR_PROJECT_ID'\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "# connect to gs://\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Connect to Google Drive \n",
        "# The program code is assumed to be on Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Set environment variable so service accounts gets access to bucket (needed for gspath)\n",
        "# (for more info see: https://cloud.google.com/docs/authentication/getting-started)\n",
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/content/gdrive/JSON_WITH_SERVICE_ACCOUNT_PRIVATE_KEYS\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6AlemFUUKzT"
      },
      "source": [
        "### ======================== RUN PARAMETERS ======================= ###\n",
        "###                                                                 ###\n",
        "# dict with bucket-region pairs\n",
        "BUCKETS = {'gs://YOUR_BUCKET_NAME/': ['US']}   \n",
        "\n",
        "# Location and type of source files (on gs://...)\n",
        "REMOTE_INPUT_FILEPATH = 'FILEPATH_TO_INPUT_FILES' # don't preface with gs://YOUR_BUCKET_NAME\n",
        "INPUT_FILE_EXTENSION = 'mp4'\n",
        "INPUT_BATCH_SIZE = 42   # number of input files to be batched into one .tfrecord file (target 400MiB .tfrecord file)\n",
        "\n",
        "# Destination where .tfrecord files will be written (on gs://...)\n",
        "DATA_DIR = 'FILEPATH_OF_TFRECORD_FILES' # don't preface with gs://YOUR_BUCKET_NAME\n",
        "\n",
        "# Local directory on backend (probably needs a High-RAM runtime type)\n",
        "LOCAL_INPUT_FILES = 'local/'\n",
        "###                                                                 ###\n",
        "### =============================================================== ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtRHxIzL6xXl"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(f\"TensorFlow v{tf.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBA4H3B8UFPd"
      },
      "source": [
        "import re\n",
        "\n",
        "# select target bucket, based on country of backend (avoid e-gress!!!)\n",
        "target_bucket = None\n",
        "for bucket, country_lst in BUCKETS.items():\n",
        "  if server_country in country_lst:\n",
        "    target_bucket = bucket\n",
        "    break\n",
        "if target_bucket is None: \n",
        "  raise ValueError(f'No target-bucket found for {server_country}')\n",
        "print(f\"Target-bucket:     {target_bucket}\")\n",
        "\n",
        "# add target-bucket to directories\n",
        "DATA_DIR = target_bucket + DATA_DIR\n",
        "REMOTE_INPUT_FILEPATH = target_bucket + REMOTE_INPUT_FILEPATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zSA1QawUQd_"
      },
      "source": [
        "# install modules used by the code\n",
        "!pip install tensorboardx\n",
        "!pip install soundfile\n",
        "!pip install tensorflow_addons\n",
        "!pip install pytube"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnhYcj3OUQbT"
      },
      "source": [
        "# Make sure python finds the imports\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/PATH_TO/audiocodec')\n",
        "sys.path.append('/content/gdrive/PATH_TO/mp4net')\n",
        "sys.path.append('/content/gdrive/PATH_TO/preprocessing')\n",
        "\n",
        "# local install of audiocodec (only needs to be executed once)\n",
        "!pip install -e /content/gdrive/PATH_TO/audiocodec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3V5BymJXM0Z"
      },
      "source": [
        "# Copy input data -> local server\n",
        "#   (only do this when data is not already on local server)\n",
        "!mkdir ./{LOCAL_INPUT_FILES}\n",
        "!gsutil -m cp {REMOTE_INPUT_FILEPATH}/* ./{LOCAL_INPUT_FILES}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxseKhhzUQOJ"
      },
      "source": [
        "# ######### #\n",
        "# DATA PREP #\n",
        "# ######### #\n",
        "#\n",
        "import datetime \n",
        "from utils import gspath\n",
        "from utils import audio_utils\n",
        "from model import mp4net\n",
        "import dataprep  \n",
        "\n",
        "in_filepath = LOCAL_INPUT_FILES\n",
        "input_file_extension = INPUT_FILE_EXTENSION\n",
        "out_filepath = DATA_DIR\n",
        "\n",
        "model = mp4net.MP4netFactory()\n",
        "\n",
        "temp_filepath = 'local_process/'\n",
        "!mkdir {temp_filepath}\n",
        "!rm {temp_filepath}*.*\n",
        "\n",
        "# group input files in batches\n",
        "file_pattern = gspath.join(in_filepath, f\"*.{input_file_extension}\")\n",
        "audio_file_paths = gspath.findall(file_pattern)\n",
        "audio_file_paths.sort()\n",
        "\n",
        "input_batch_size = INPUT_BATCH_SIZE\n",
        "input_files_batched = [audio_file_paths[i:i + input_batch_size] \n",
        "                       for i in range(0, len(audio_file_paths), input_batch_size)]\n",
        "\n",
        "# loop over batches\n",
        "for batch_no, batch in enumerate(input_files_batched):\n",
        "  print()\n",
        "  print(f'batch {batch_no}')\n",
        "  tf_output_filename = gspath.join(out_filepath, f'yt-{batch_no:04d}' + f'_sr{model.sample_rate}_Nx{model.freq_n}x{model.channels_n}.tfrecord')\n",
        "\n",
        "  if gspath.findall(tf_output_filename):\n",
        "    # skip if output file already exists (maybe from earlier run that crashed)\n",
        "    print(f'  Output file {tf_output_filename} already exists...')\n",
        "  else:\n",
        "    # loop over all songs in batch\n",
        "    temp_wavs = []\n",
        "    for song_no, song_filename in enumerate(batch):\n",
        "      # convert and resample to WAV\n",
        "      temp_wavfile = temp_filepath + f'yt-{batch_no:04d}-{song_no:02d}.wav'   \n",
        "      temp_wavs.append(temp_wavfile)\n",
        "      print(f'  resampling to {model.sample_rate}Hz: {song_filename} -> {temp_wavfile}')\n",
        "\n",
        "      !ffmpeg -loglevel quiet -i {song_filename} -ar {model.sample_rate} {temp_wavfile} \n",
        "\n",
        "    # loop over all songs in batch\n",
        "    print(f\"  {datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S.%f')}: {tf_output_filename} <-- {temp_wavs}\")\n",
        "\n",
        "    # convert to tf-record\n",
        "    dataprep.audio2tfrecord(temp_wavs, tf_output_filename, model)\n",
        "\n",
        "  !rm {temp_filepath}*.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRrO74PeUQCg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}