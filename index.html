<!DOCTYPE html>
<html lang="en">

<head>
    <title>MelNet</title>

    <link href="https://fonts.googleapis.com/css?family=Crimson+Text:400,400i,600" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="css/blog.css">
    <link rel="stylesheet" type="text/css" href="css/player.css">
    <link rel="stylesheet" type="text/css" href="css/index.css">

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<body>

    <div class='header'>
        <h1>MelNet</h1>
        <div class='line'>
        <div id='subtitle'>A Generative Model for Audio in the Frequency Domain</div>
        <div id='date'>June 5, 2019</div>
        </div>
    </div>

    <div id='first' class='full-width'>



        <div class='intro-demo'>
            <div class='intro'>
                <p>
                    Existing generative models for audio have predominantly aimed to directly model time-domain waveforms. MelNet instead aims to model the frequency content of an audio signal. MelNet can be used to model audio unconditionally, making it capable of tasks such as music generation. It can also be conditioned on text and speaker, making it applicable to tasks such as text-to-speech and voice conversion.  The full paper is available on arXiv: <a href='https://arxiv.org/abs/1906.01083' target="_blank">https://arxiv.org/abs/1906.01083</a>.
                </p>
                <p>
                    <strong>Demo:</strong> Select a speaker and a sentence to view a spectrogram generated by MelNet. Play the audio to visualize how the frequencies change over time.
                </p>
            </div>



            <div id='demo-3' class='demo'>




                <div id='player-3' class='player'>
                <div class='controls'>
                    <p>
                        Speaker:
                        <select id='select-speaker'>
                          <option value="BillGates">Bill Gates</option>
                          <option value="DaphneKoller">Daphne Koller</option>
                          <option value="FeiFeiLi">Fei-Fei Li</option>
                          <option value="GeorgeTakei">George Takei</option>
                          <option value="JaneGoodall">Jane Goodall</option>
                          <option value="SalmanKhan">Sal Khan</option>
                          <option value="StephenWolfram">Stephen Wolfram</option>
                          <option value="StephenHawking">Stephen Hawking</option>
                        </select>
                    </p>
                    <p>
                        Text:
                        <select id='select-text'>
                          <option value="0">A cramp is no small danger on a swim.</option>
                          <option value="1">He said the same phrase thirty times.</option>
                          <option value="2">Pluck the bright rose without leaves.</option>
                          <option value="3">Two plus seven is less than ten.</option>
                          <option value="4">The glow deepened in the eyes of the sweet girl.</option>
                          <option value="5">Bring your problems to the wise chief.</option>
                          <option value="6">Write a fond note to the friend you cherish.</option>
                          <option value="7">Clothes and lodging are free to new men.</option>
                          <option value="8">We frown when events take a bad turn.</option>
                          <option value="9">Port is a strong wine with a smoky taste.</option>
                        </select>
                    </p>
                </div>

                    <div class='spectrogram'>
                        <div class='overlay'></div>
                        <div class='underlay'>
                            <img src='samples/figs/ted_speakers/BillGates/sample-0.png'>
                        </div>
                    </div>


                    <div class='audio-controls'>
                        <button id="playpause" disabled class='playpause' title="play">
                            <svg class='play-img' width="14px" height="19px" viewBox="0 0 14 19">
                                <polygon id="Triangle" fill="#000000" transform="translate(9, 9.5) rotate(90) translate(-7, -9.5) " points="7 2.5 16.5 16.5 -2.5 16.5"></polygon>
                            </svg>
                            <svg class='pause-img' width="16px" height="19px" viewBox="0 0 16 19">
                                <g fill="#000000" stroke="#000000">
                                    <rect id="Rectangle" x="0.5" y="0.5" width="4" height="18"></rect>
                                    <rect id="Rectangle" x="11.5" y="0.5" width="4" height="18"></rect>
                                </g>
                            </svg>
                        </button>

                        <audio id='play-1' class='play'>
                            <source id='src1' src='samples/figs/ted_speakers/BillGates/sample-0.ogg' type='audio/ogg'>
                            <source id='src2' src='samples/figs/ted_speakers/BillGates/sample-0.wav' type='audio/wav'>
                        </audio>
                        <div class='response'>
                            <canvas class='response-canvas'></canvas>
                        </div>
                    </div>

                </div>
            </div>
        </div>
    </div>


    <div class='section'>
        <h2>The Frequency Domain</h2>
       <!--  <p>
        A brief primer on time and frequency representations of audio.
        </p>
        <h3>Background: The Frequency Domain</h3>
        <p> -->
        <p>
        We begin with a brief primer on time and frequency representations of audio.
        A signal has equivalent representations in the time domain and the frequency domain.  This notion is pervasive in the analysis of highly periodic signals such as audio waveforms.  In the time domain, audio is represented as a one-dimensional waveform&mdash;amplitude as a function of time.  By way of the Fourier transform, this waveform can be decomposed into a sum of periodic signals.  The amplitudes and frequencies of these periodic signals constitute the waveform's frequency-domain representation, which is visualized in the animation below:
        </p>

        <div class='fig-container'>
            <figure id='fourier-div'>
                <img id='fourier' src='../../figures/fourier4.gif'>
            </figure>

            <figcaption>
                <p class='caption'>
                A visualization of the relationship between a function in the <span style='color: rgb(189, 41, 32);'>time domain</span> and the <span style='color: rgb(37, 94, 182);'>frequency domain</span>. The time-domain waveform can be represented as a weighted sum of periodic functions of varying frequencies. [<a target='_blank' href='https://en.wikipedia.org/wiki/Frequency_domain#/media/File:Fourier_transform_time_and_frequency_domains_(small).gif'>Image source</a>]
                </p>
            </figcaption>
        </div>

        <h3>Time-Frequency Representations</h3>
        <p>
        MelNet models spectrograms, which are time-frequency representations of audio.  Time-frequency representations lie in-between the time and frequency domains. They preserve the high-level temporal structure of the time-domain signal, but locally apply a transformation to the frequency domain.  The resulting two-dimensional representation can be thought of as a sequence of frequency responses.  Time-frequency representations highlight how the tones and pitches within an audio signal vary through time. Intuitively, such a representation could be helpful for modelling data such as music, as salient structure such as notes are easier to observe and ostensibly easier to model.  To further align our representations with human perception, we transform the frequency axis of the spectrogram to the Mel scale.  The Mel scale is designed to emphasize frequencies which are more important to human perception.
        </p>
        <p>
        In addition to being well-aligned with human perception, spectrograms offer potential benefits from a modelling standpoint.  Modelling long-range dependencies in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps in the time domain.  However, the same signal spans only several hundreds of timesteps in spectrograms, potentially easing the task of capturing long-range temporal dependencies.
        </p>

        <div class='fig-container'>
            <figure id='representations'>
                <p class='repr-caption'>Mel-spectrogram representation</p>
                <div id='mel-container'>
                    <img id='under' src='../../figures/spectrogram.png'>
                    <div id='over-left'></div>
                    <div id='over-right'></div>
                </div>
                <p class='repr-caption'>Waveform representation</p>
                <div id='waveform-container'>
                    <img id='waveform' src='../../figures/waveform1x.svg'>
                </div>
                <div class='buttons' id='zooms'>
                    <button id='zoom1'  name='1' class='zoom zoom-selected'>1x</button>
                    <button id='zoom2'  name='5' class='zoom'>5x</button>
                    <button id='zoom3'  name='25' class='zoom'>25x</button>
                    <button id='zoom4'  name='125' class='zoom'>125x</button>
                </div>
            </figure>

            <figcaption>
                <p class='caption'>
                Spectrogram and waveform representations of the same four-second audio signal. The waveform spans nearly 100,000 timesteps whereas the temporal axis of the spectrogram spans roughly 400.  Complex structure is nested within the temporal axis of the waveform at various timescales, whereas the spectrogram has structure which is smoothly spread across the time-frequency plane.
                </p>
            </figcaption>
        </div>

    </div>

    <hr>

    <div class='section'>
        <h2>Model</h2>
        <p>
        Spectrograms have been the preferred representations for the vast majority of discriminative tasks involving audio, so it seems natural to use spectrograms for generative modelling as well.  To model the distribution over spectrograms, we devise a highly expressive model which synthesizes many recent advances in autoregressive modelling.
        </p>

        <h3>Autoregressive Modelling</h3>
        <p>
        Autoregressive models learn to model complex, high-dimensional distributions by modelling a sequence of simpler distributions.
        This approach, whereby a model is trained to predict one element at a time, has been successfully applied to a wide variety of data modalities&mdash;including images, text, and waveforms.  MelNet applies this same approach to the modelling of spectrograms.  Much like autoregressive image models such as PixelCNN estimate a distribution pixel-by-pixel over the spatial dimensions of an image, MelNet estimates a distribution element-by-element over the time and frequency dimensions of a spectrogram.
        </p>
        <p>
        We experiment with two different autoregressive orderings. The first is a simple time-major ordering which proceeds through each spectrogram frame from low to high frequency, before progressing to the next frame.  The second is a multiscale ordering which is visualized below and further described in the following section.
        </p>

        <div class='fig-container'>
            <figure id='ar'>
                <img id='ar-img'  src='../../figures/autoregressive.gif'>
            </figure>

            <figcaption>
                <p class='caption'>
                Autoregressive generation of spectrograms using a time-major ordering (left) and a multiscale ordering (right).  For visualization purposes, the animation generates a 8x8 patch, rather than a single element,  at each timestep.
                </p>
            </figcaption>
        </div>

        <h3>Multiscale Modelling</h3>
        <p>
        One drawback of autoregressive models is that they tend to learn local structure much better than global structure.  When modelling images with a standard row-major ordering, this results in generated samples which have realistic textures but lack coherent high-level structure at the level of objects and higher-level scene composition.  This problem is particularly noticeable when modelling high-dimensional distributions.  Since we aim to model spectrograms with hundreds of thousands of dimensions, it's essential to introduce measures to counteract these effects.  To this end, we use a multiscale model which generates spectrograms in a coarse-to-fine order.  A low-resolution, subsampled spectrogram that captures high-level structure is generated initially, followed by an iterative upsampling procedure that adds high-resolution details.  By generating spectrograms in this manner, it is possible to decouple the tasks of learning local and global structure.
        </p>

        <div class='fig-container'>
            <figure id='multiscale-fig'>
                <div id='multiscale'>
                    <div class='tier-container' id = 'tier-container0'>
                        <img class='tier' id='tier0' src='../../figures/sample-7-tier-0.png'>
                    </div>
                    <div class='tier-container' id = 'tier-container1'>
                        <img class='tier' id='tier1' src='../../figures/sample-7-tier-1.png'>
                    </div>
                    <div class='tier-container' id = 'tier-container2'>
                        <img class='tier' id='tier2' src='../../figures/sample-7-tier-2.png'>
                    </div>
                    <div class='tier-container' id = 'tier-container3'>
                        <img class='tier' id='tier3' src='../../figures/sample-7-tier-3.png'>
                    </div>
                    <div class='tier-container' id = 'tier-container4'>
                        <img class='tier' id='tier4' src='../../figures/sample-7-tier-4.png'>
                    </div>
                    <div class='tier-container' id = 'tier-container5'>
                        <img class='tier' id='tier5' src='../../figures/sample-7-tier-5.png'>
                    </div>
                </div>
                <div class='buttons' id='tier-buttons'>
                    <button class='change-tier' id='tier-bw'>Previous</button>
                    <button class='change-tier' id='tier-fw'>Next</button>
                </div>
            </figure>

            <figcaption>
                <p class='caption'>
                A sampled spectrogram viewed at different stages of the multiscale generation procedure. The initial tier dictates high-level structure and subsequent tiers add fine-grained details.  Each upsampling tier doubles the resolution of the spectrogram, resulting in the initial tier being upsampled by a factor of 32.
                </p>
            </figcaption>
        </div>
    </div>

    <hr>

    <div class='section'>
        <h2>Unconditional Audio Generation</h2>
        <p>
        For the task of unconditional audio generation, we train MelNet on sequences of unlabelled audio. Unconditional generative models have demonstrated impressive results for images and text&mdash;GANs have been able to generate photorealistic faces and language models have generated realistic prose.  However, unconditional generative models for audio have arguably been less successful at generating realistic samples.  Nonetheless, unconditional generation is an insightful task since generated samples offer insights into the structures that a model has learned.  A model trained on speech should learn to generate phonemes, words, and higher levels of linguistic structure.  A model trained on music should learn to generate basic musical structures such as notes, chords, melody, and rhythm.
        </p>
        <h3>Experiments</h3>
        <p>
        We train MelNet to generate audio unconditionally using three diverse datasets:
        <ul>
            <li><strong>Music:</strong> We use a dataset of solo piano performances recorded as part of an international piano competition.  The data was recorded over several years, resulting in year-to-year variations in recording conditions.</li>
            <li><strong>Single-Speaker:</strong> We use a speech dataset consisting of a single speaker reading audiobooks in a quiet environment.  The books are read in a highly expressive manner, including significant variation in intonation and prosody as well as various character voices.</li>
            <li><strong>Multi-Speaker:</strong> We use a multi-speaker, multilingual speech dataset which contains speech from speakers of 145 different nationalities, covering a wide range of accents, ages, ethnicities and languages.</li>
        </ul>
        </p>
    </div>

    <div class='full-width'>
        <div id='demo-1' class='demo'>

<!--             <div class='controls'>
                <p>
                    Dataset:
                    <select id='select-dataset'>
                      <option value="maestro">Music</option>
                      <option value="blizzard">Single-Speaker</option>
                      <option value="voxceleb">Multi-Speaker</option>
                    </select>
                </p>

                <p>
                    Sample:
                    <select id='select-sample'>
                      <option value="0">Sample 1</option>
                      <option value="1">Sample 2</option>
                      <option value="2">Sample 3</option>
                      <option value="3">Sample 4</option>
                      <option value="4">Sample 5</option>
                      <option value="5">Sample 6</option>
                    </select>
                </p>
            </div>
 -->

            <div id='player-1' class='player'>

            <div class='controls'>
                <p>
                    Dataset:
                    <select id='select-dataset'>
                      <option value="maestro">Music</option>
                      <option value="blizzard">Single-Speaker</option>
                      <option value="voxceleb2">Multi-Speaker</option>
                    </select>
                </p>

                <p>
                    Sample:
                    <select id='select-sample'>
                      <option value="0">Sample 1</option>
                      <option value="1">Sample 2</option>
                      <option value="2">Sample 3</option>
                      <option value="3">Sample 4</option>
                      <option value="4">Sample 5</option>
                      <option value="5">Sample 6</option>
                      <option value="6">Sample 7</option>
                      <option value="7">Sample 8</option>
                    </select>
                </p>
            </div>

                <div class='spectrogram'>
                    <div class='overlay'></div>
                    <div class='underlay'>
                        <img src='samples/figs/maestro/sample-0.png'>
                    </div>
                </div>




                <div class='audio-controls'>
                    <button id="playpause" disabled class='playpause' title="play">
                        <svg class='play-img' width="14px" height="19px" viewBox="0 0 14 19">
                            <polygon id="Triangle" fill="#000000" transform="translate(9, 9.5) rotate(90) translate(-7, -9.5) " points="7 2.5 16.5 16.5 -2.5 16.5"></polygon>
                        </svg>
                        <svg class='pause-img' width="16px" height="19px" viewBox="0 0 16 19">
                            <g fill="#000000" stroke="#000000">
                                <rect id="Rectangle" x="0.5" y="0.5" width="4" height="18"></rect>
                                <rect id="Rectangle" x="11.5" y="0.5" width="4" height="18"></rect>
                            </g>
                        </svg>
                    </button>
                    <audio id='play-1' class='play'>
                        <source id='src1' src='samples/figs/maestro/sample-0.ogg' type='audio/ogg'>
                        <source id='src2' src='samples/figs/maestro/sample-0.wav' type='audio/wav'>
                    </audio>
                    <div class='response'>
                        <canvas class='response-canvas'></canvas>
                    </div>
                </div>

            </div>

        </div>


    </div>

    <div class='section'>
        <h2>Text-to-Speech</h2>
        <p>
        While generating samples from unconditional distributions is insightful, it's not particularly useful.  Conditional generation tasks, on the other hand, generally have more practical applications.  Examples of conditional generation tasks in other domains include image inpainting, super-resolution,  and language translation.  To demonstrate a practical application for MelNet, we apply it to the task of text-to-speech, i.e. text-conditional audio generation.  We also demonstrate that MelNet can be conditioned on speakers, allowing it to jointly learn the speaking characteristics of thousands of individuals.
        </p>

        <h3>Learned Alignment</h3>
        <p>
        To learn text-to-speech in an end-to-end manner, MelNet learns a latent alignment between the audio and text, which is represented as a sequence of characters.  To accomplish this, we add an attention mechanism which, at each timestep, emits a distribution over the character sequence which effectively selects a set of characters which are relevant for pronouncing the next word.  Below is an example of a learned alignment between a spectrogram and text.
        </p>
        <div class='fig-container'>
            <figure id='phi'>
                <div id='phi-fig'>
                    <img  id='image2' src='../../figures/phi.png'>
                </div>
            </figure>

            <figcaption>
                <p class='caption'>
                Learned alignment between a spectrogram and the character sequence &ldquo;that we mother nature thank you&rdquo;.  The columns correspond to the learned attention distributions for each timestep.  The text is read with long, deliberate pauses which appear as flat regions in the alignment.
                </p>
            </figcaption>
        </div>
        <p>
        MelNet learns a fairly sparse attention distribution&mdash;at each timestep it only attends to a few recent characters and ignores the rest.  This is consistent with the way in which humans read text, as each phoneme can generally be deduced from a small neighborhood of adjacent characters.
        </p>
        <h3>Experiments</h3>
        <p>
        We train MelNet to perform text-to-speech using a single-speaker dataset and a multi-speaker dataset:
        <ul>
            <li><strong>Single-Speaker:</strong> We reuse the single-speaker audiobook dataset used for the unconditional speech generation task, but also use the transcripts during training.</li>
            <li><strong>Multi-Speaker:</strong> We use a multi-speaker dataset with roughly 2,000 different speakers (~10 minutes per speaker) with significant variation in speaking styles and recording conditions.  The transcripts for this dataset are fairly noisy&mdash;there are various transcription errors, the text is unnormalized, and capitalization and punctuation are omitted. As a result, the model must infer proper intonation and prosody from the semantic content of the text.</li>
        </ul>
        </p>
    </div>


    <div class='full-width'>
        <div id='demo-2' class='demo'>

<!--
            <div class='mel'>
                <div class='overlay'></div>
                <div class='underlay'>
                    <img src='samples/figs/ted/sample-0.png'>
                </div>
            </div>

            <div class='player-response'>
                <div class='response-container'>
                <div class='response'>
                    <div class='canvas-container'>
                        <canvas id='canvas'></canvas>
                    </div>
                </div>
            </div>
            </div> -->


<!--             <div class='controls'>
                <p>
                    Dataset:
                    <select id='select-dataset2'>
                      <option value="blizzard">Single-Speaker</option>
                      <option value="ted">Multi-Speaker</option>
                    </select>
                </p>
                <p>
                    Sample:
                    <select id='select-sample2'>
                      <option value="0">Sample 1</option>
                      <option value="1">Sample 2</option>
                      <option value="2">Sample 3</option>
                      <option value="3">Sample 4</option>
                      <option value="4">Sample 5</option>
                      <option value="5">Sample 6</option>
                      <option value="6">Sample 7</option>
                    </select>
                </p>
            </div> -->

            <div id='player-2' class='player'>

            <div class='controls'>
                <p>
                    Dataset:
                    <select id='select-dataset2'>
                      <option value="blizzard">Single-Speaker</option>
                      <option value="ted">Multi-Speaker</option>
                    </select>
                </p>
                <p>
                    Sample:
                    <select id='select-sample2'>
                      <option value="0">Sample 1</option>
                      <option value="1">Sample 2</option>
                      <option value="2">Sample 3</option>
                      <option value="3">Sample 4</option>
                      <option value="4">Sample 5</option>
                      <option value="5">Sample 6</option>
                      <option value="6">Sample 7</option>
                      <!-- <option value="7">Sample 8</option>
                      <option value="8">Sample 9</option>
                      <option value="9">Sample 10</option> -->
                    </select>
                </p>
            </div>


                <p id='sample-text'>&ldquo;My dear Fanny, you feel these things a great deal too much. I am most happy that you like the chain,&rdquo;</p>

                <div class='spectrogram'>
                    <div class='overlay'></div>
                    <div class='underlay'>
                        <img src='samples/figs/blizzard_tts/sample-0.png'>
                    </div>
                </div>


                <div class='audio-controls'>
                    <button id="playpause" disabled class='playpause' title="play">
                        <svg class='play-img' width="14px" height="19px" viewBox="0 0 14 19">
                            <polygon id="Triangle" fill="#000000" transform="translate(9, 9.5) rotate(90) translate(-7, -9.5) " points="7 2.5 16.5 16.5 -2.5 16.5"></polygon>
                        </svg>
                        <svg class='pause-img' width="16px" height="19px" viewBox="0 0 16 19">
                            <g fill="#000000" stroke="#000000">
                                <rect id="Rectangle" x="0.5" y="0.5" width="4" height="18"></rect>
                                <rect id="Rectangle" x="11.5" y="0.5" width="4" height="18"></rect>
                            </g>
                        </svg>
                    </button>
                    <audio id='play-1' class='play'>
                        <source id='src1' src='samples/figs/blizzard_tts/sample-0.ogg' type='audio/ogg'>
                        <source id='src2' src='samples/figs/blizzard_tts/sample-0.wav' type='audio/wav'>
                    </audio>
                    <div class='response'>
                        <canvas class='response-canvas'></canvas>
                    </div>
                </div>

            </div>

        </div>
    </div>

    <div class='section'>
        <h2>Conclusion</h2>
        <p>
        MelNet combines various representational and modelling improvements to yield a highly expressive, broadly applicable, and fully end-to-end generative model of audio.  We believe MelNet presents a promising direction for future work across a wide variety of audio generation tasks.  Furthermore, MelNet is able to uncover salient structure from large quantities of unlabelled audio, suggesting future applications to unsupervised representation learning for audio.
        </p>
    </div>






</body>
</html>

<script src="js/pako.js"></script>
<script src="js/upng.js"></script>
<script src="js/player.js"></script>

<script type="text/javascript">

    function tier_resize() {
        for (let i = 0; i < 6; i++) {
            var c = document.getElementById('tier-container' + i.toString())
            var img = document.getElementById('tier' + i.toString())
            img.style.width = c.parentElement.offsetWidth + 'px'
        }
    }

    tier_resize()
    window.addEventListener("resize", tier_resize)

    var tier = 0

    function state() {
        if (tier == 5) {
            document.getElementById('tier-fw').className = 'change-tier change-tier-selected'
        } else {
            document.getElementById('tier-fw').className = 'change-tier'
        }


        if (tier == 0) {
            document.getElementById('tier-bw').className = 'change-tier change-tier-selected'
        } else {
            document.getElementById('tier-bw').className = 'change-tier'
        }

    }

    function change(prev_tier, next_tier) {
        next_tier = Math.max(Math.min(next_tier, 5), 0)
        if (prev_tier < next_tier) {
            document.getElementById('tier-container' + next_tier.toString()).style.width = '100%'
        } else if (prev_tier > next_tier) {
            document.getElementById('tier-container' + prev_tier.toString()).style.width = '0%'
        }
        tier = next_tier
        state()







    }

    document.getElementById('tier-fw').onclick = () => {tier += 1; change(tier - 1, tier)}
    document.getElementById('tier-bw').onclick = () => {tier -= 1; change(tier + 1, tier)}


    document.getElementById('tier-fw').onmousedown = () => {document.getElementById('tier-fw').className = 'change-tier change-tier-selected';}
    document.getElementById('tier-fw').onmouseup = () => {if(tier <=4) {document.getElementById('tier-fw').className = 'change-tier';}}
    document.getElementById('tier-fw').onmouseout = () => {if(tier <=4) {document.getElementById('tier-fw').className = 'change-tier';}}

    document.getElementById('tier-bw').onmousedown = () => {document.getElementById('tier-bw').className = 'change-tier change-tier-selected';}
    document.getElementById('tier-bw').onmouseup = () => {if(tier >=1) {document.getElementById('tier-bw').className = 'change-tier';}}
    document.getElementById('tier-bw').onmouseout = () => {if(tier >=1) {document.getElementById('tier-bw').className = 'change-tier';}}




</script>

<script type="text/javascript">
    var button = document.getElementById('zoom1')
    var button2 = document.getElementById('zoom2')
    var button3 = document.getElementById('zoom3')
    var button4 = document.getElementById('zoom4')
    var waveform = document.getElementById('waveform')

    var left = document.getElementById('over-left')
    var right = document.getElementById('over-right')

    var selected = button

    function update() {

        var scale = this.name

        var prev_selected = selected
        selected = this

        prev_selected.className = 'zoom'
        selected.className = 'zoom zoom-selected'

        // prev_selected.style['background-color:hover'] = 'rgba(0, 0, 0, 0.04)'

        var width = 100*((parseInt(scale) - 1) / (2*(parseInt(scale))))
        left.style.width = width.toString() + '%'
        right.style.width = width.toString() + '%'


        right.addEventListener("transitionend", function() {
            waveform.src='../../figures/waveform' + scale + 'x.svg'
        });
    }

    button.onclick = update
    button2.onclick = update
    button3.onclick = update
    button4.onclick = update

</script>

<script>
    var p1 = new Player('player-1')
    p1.load('samples/figs/maestro/sample-0', 'samples/figs/maestro/sample-0.png')

    var select_dataset = document.querySelector('#select-dataset')
    var select_sample = document.querySelector('#select-sample')

    function update() {
        var audio_fname = "samples/figs/" + select_dataset.value + "/sample-" + select_sample.value
        var img_fname = "samples/figs/" + select_dataset.value + "/sample-" + select_sample.value + ".png"
        p1.load(audio_fname, img_fname)
    }
    select_dataset.addEventListener('change', update)
    select_sample.addEventListener('change', update)
    window.addEventListener("resize", function() {p1.redraw()})
</script>

<script>
    ted_sents = [
        "it wasn't like i was asking for the code to a nuclear bunker or anything like that but the amount of resistance i got from this",
        "and no matter what the rest of the world tells them they should be",
        "chances are that they are rooted in the productivity crisis",
        "and we were down to eating one meal a day running from place to place but wherever we could help we did at a certain point in time in",
        "syria was largely a place of tolerance historically accustomed",
        "the years went by and the princess grew up into a beautiful young woman",
        "and when they came back and told us about it we really started thinking about the ways in which we see styrofoam every day",
        "i spent so much time learning this language why do i only",
        "phrases and words even if you have a phd of chinese language you can't understand them",
        "that every person here every decision that you've made today every decision you've made in your life you've not really made that decision but in fact",
        // "and what that form is modeling and shaping is not cement",
        // "is only a very recent religious enthusiasm it surfaced only in the west",
        // "i cannot face your fears or chase your dreams and you can't do that for me but we can be supportive of eachother",
        // "the first law of travel and therefore of life youre only as strong",
        // "it is the earth as we know it it is seen from in this case from outside the orbit of saturn",
    ]
    blizzard_sents = [
        "&ldquo;My dear Fanny, you feel these things a great deal too much. I am most happy that you like the chain,&rdquo;",
        "&ldquo;I saw his fallibilities: I comprehended them.&rdquo;",
        "Lydia was Lydia still; untamed, unabashed, wild, noisy, and fearless.",
        "Looking with a half fantastic curiosity to see whether the tender grass of early spring,",
        "Which a too curious reflection was apt to disturb in its ecclesiastical and military compeers. For once medievalism and modernism had a common stand-point.",
        "&ldquo;I like them round,&rdquo; said Mary. &ldquo;And they are exactly the color of the sky over the moor.&rdquo;",
        "&ldquo;Oh, he has been away from New York&mdash;he has been all round the world. He doesn't know many people here, but he's very sociable, and he wants to know every one.&rdquo;",
    ]

    var p2 = new Player('player-2')
    p2.load('samples/figs/blizzard_tts/sample-0', 'samples/figs/blizzard_tts/sample-0.png')

    var select_sample2 = document.querySelector('#select-sample2')
    var select_dataset2 = document.querySelector('#select-dataset2')
    var sample_text = document.querySelector('#sample-text')

    function update2() {
        if (select_dataset2.value == 'ted') {
            var text = ted_sents[select_sample2.value]
            var audio_fname = "samples/figs/ted/sample-" + select_sample2.value
            var img_fname = "samples/figs/ted/sample-" + select_sample2.value + ".png"
        } else {
            var text = blizzard_sents[select_sample2.value]
            var audio_fname = "samples/figs/blizzard_tts/sample-" + select_sample2.value
            var img_fname = "samples/figs/blizzard_tts/sample-" + select_sample2.value + ".png"
        }
        p2.load(audio_fname, img_fname)
        sample_text.innerHTML = text
    }
    select_dataset2.addEventListener('change', update2)
    select_sample2.addEventListener('change', update2)
    window.addEventListener("resize", function() {p2.redraw()})
</script>

<script>
    var p3 = new Player('player-3')
    p3.load('samples/figs/ted_speakers/BillGates/sample-0', 'samples/figs/ted_speakers/BillGates/sample-0.png')

    var select_text = document.querySelector('#select-text')
    var select_speaker = document.querySelector('#select-speaker')

    function update3() {
        var audio_fname = "samples/figs/ted_speakers/" + select_speaker.value + "/sample-" + select_text.value
        var img_fname = "samples/figs/ted_speakers/" + select_speaker.value + "/sample-" + select_text.value + ".png"
        p3.load(audio_fname, img_fname)
    }
    select_text.addEventListener('change', update3)
    select_speaker.addEventListener('change', update3)
    window.addEventListener("resize", function() {p3.redraw()})
</script>